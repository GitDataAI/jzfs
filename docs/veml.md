# VeML: An End-to-End Machine Learning Lifecycle for Large-scale and High-dimensional Data

## ABSTRACT

An end-to-end machine learning (ML) lifecycle consists of many iterative processes, from data preparation and ML model design to model training and then deploying the trained model for inference. When building an end-to-end lifecycle for an ML problem, many ML pipelines must be designed and executed that produce a huge number of lifecycle versions. Therefore, this paper introduces VeML, a Version management system dedicated to end-to-end ML Lifecycle. Our system tackles several crucial problems that other systems have not solved. First, we address the high cost of building an ML lifecycle, especially for large-scale and high-dimensional dataset. We solve this problem by proposing to transfer the lifecycle of similar datasets managed in our system to the new training data. We design an algorithm based on the core set to compute similarity for large-scale, high-dimensional data efficiently. Another critical issue is the model accuracy degradation by the difference between training data and testing data during the ML lifetime, which leads to lifecycle rebuild. Our system helps to detect this mismatch without getting labeled data from testing data and rebuild the ML lifecycle for a new data version. To demonstrate our contributions, we conduct experiments on real-world, large-scale datasets of driving images and spatiotemporal sensor data and show promising results.

ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ (ML)ç”Ÿå‘½å‘¨æœŸåŒ…æ‹¬è®¸å¤šè¿­ä»£è¿‡ç¨‹ï¼šä»æ•°æ®å‡†å¤‡ã€MLæ¨¡å‹è®¾è®¡åˆ°æ¨¡å‹è®­ç»ƒï¼Œç„¶åéƒ¨ç½²è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚  

åœ¨ä¸ºMLé—®é¢˜æ„å»ºç«¯åˆ°ç«¯çš„ç”Ÿå‘½å‘¨æœŸæ—¶ï¼Œå¿…é¡»è®¾è®¡å’Œæ‰§è¡Œè®¸å¤šMLç®¡é“ï¼Œè¿™äº›ç®¡é“ä¼šäº§ç”Ÿå¤§é‡çš„ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ã€‚  

å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†VeMLï¼Œä¸€ä¸ªè‡´åŠ›äºç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸçš„ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿã€‚  

æˆ‘ä»¬çš„ç³»ç»Ÿè§£å†³äº†å…¶ä»–ç³»ç»Ÿæ²¡æœ‰è§£å†³çš„å‡ ä¸ªå…³é”®é—®é¢˜ã€‚  

é¦–å…ˆï¼Œæˆ‘ä»¬è§£å†³äº†æ„å»ºæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸçš„é«˜æˆæœ¬é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§è§„æ¨¡å’Œé«˜ç»´æ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºå°†ç³»ç»Ÿä¸­ç®¡ç†çš„ç±»ä¼¼æ•°æ®é›†çš„ç”Ÿå‘½å‘¨æœŸè½¬ç§»åˆ°æ–°çš„è®­ç»ƒæ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†é«˜æ•ˆåœ°è®¡ç®—å¤§è§„æ¨¡ã€é«˜ç»´æ•°æ®çš„ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ ¸å¿ƒé›†çš„ç®—æ³•ã€‚  

å¦ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯ï¼Œåœ¨æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸä¸­ï¼Œè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¹‹é—´çš„å·®å¼‚ä¼šå¯¼è‡´æ¨¡å‹ç²¾åº¦ä¸‹é™ï¼Œä»è€Œå¯¼è‡´ç”Ÿå‘½å‘¨æœŸé‡å»ºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå¸®åŠ©æ£€æµ‹è¿™ç§ä¸åŒ¹é…ï¼Œè€Œæ— éœ€ä»æµ‹è¯•æ•°æ®ä¸­è·å–æ ‡è®°æ•°æ®ï¼Œå¹¶ä¸ºæ–°æ•°æ®ç‰ˆæœ¬é‡å»ºMLç”Ÿå‘½å‘¨æœŸã€‚  

ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„è´¡çŒ®ï¼Œæˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡é©¾é©¶å›¾åƒå’Œæ—¶ç©ºä¼ æ„Ÿå™¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶æ˜¾ç¤ºäº†æœ‰å¸Œæœ›çš„ç»“æœã€‚  


## Introduction

FIRSTLY, we try to answer the question: why do we need a version management system for the end-to-end ML lifecycle?  When building an end-to-end ML lifecycle, we need to deal with many possible choices for data preparation, ML algorithms, training hyper-parameters, and deployment configurations.  As a results, it costs huge time and computation to build an end-to-end ML lifecycle.  Moreover, the ML task continuously evolves throughout its lifetime that produces a a lot of lifecycle versions, from data versions to inference versions.  Therefore, we built our Version management system dedicated to the end-to-end ML lifecycle (VeML) to manage many ML lifecycle versions and leverage the stored versions for efficiently building a new ML lifecycle.  Figure 1 shows the data flow of our system from the data collection through our ML version management to model serving and go back with the new data.

é¦–å…ˆï¼Œæˆ‘ä»¬è¯•å›¾å›ç­”è¿™ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªç«¯åˆ°ç«¯MLç”Ÿå‘½å‘¨æœŸçš„ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿï¼Ÿ  

åœ¨æ„å»ºç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†è®¸å¤šå¯èƒ½çš„é€‰æ‹©ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æœºå™¨å­¦ä¹ ç®—æ³•ã€è®­ç»ƒè¶…å‚æ•°å’Œéƒ¨ç½²é…ç½®ã€‚å› æ­¤ï¼Œæ„å»ºç«¯åˆ°ç«¯çš„MLç”Ÿå‘½å‘¨æœŸéœ€è¦èŠ±è´¹å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—ã€‚  

æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ ä»»åŠ¡åœ¨å…¶æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­ä¸æ–­å‘å±•ï¼Œäº§ç”Ÿäº†è®¸å¤šç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ï¼Œä»æ•°æ®ç‰ˆæœ¬åˆ°æ¨ç†ç‰ˆæœ¬ã€‚  

å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸“ç”¨äºç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸ(VeML)çš„ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿï¼Œä»¥ç®¡ç†è®¸å¤šæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ï¼Œå¹¶åˆ©ç”¨å­˜å‚¨çš„ç‰ˆæœ¬æœ‰æ•ˆåœ°æ„å»ºæ–°çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚

å›¾1æ˜¾ç¤ºäº†ç³»ç»Ÿçš„æ•°æ®æµï¼Œä»æ•°æ®æ”¶é›†åˆ°MLç‰ˆæœ¬ç®¡ç†ï¼Œå†åˆ°æ¨¡å‹æœåŠ¡ï¼Œç„¶åè¿”å›æ–°æ•°æ®ã€‚

![](./img/veml.jpg)

å›¾1ï¼šç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸçš„ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿæ•°æ®æµã€‚

In this paper, we raise some crucial research questions for an end-to-end ML lifecycle management system that existing systems do not fully solve. We will show that our proposed VeML system can tackle these challenges in one unified system.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†ç³»ç»Ÿæå‡ºäº†ä¸€äº›å…³é”®çš„ç ”ç©¶é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ˜¯ç°æœ‰ç³»ç»Ÿæ— æ³•å®Œå…¨è§£å†³çš„ã€‚æˆ‘ä»¬å°†å±•ç¤ºæˆ‘ä»¬æå‡ºçš„VeMLç³»ç»Ÿå¯ä»¥åœ¨ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿä¸­è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚

The first challenge for an ML lifecycle management system is how to manage a huge number of versions in an end-to-end ML lifecycle. Our system is built from ground on an internal in-memory storage engine for large-scale storage, integrating an enterprise-strength graph database like Neo4j [31] for graph-based lifecycle versions management, and a unified ML training framework, OpenMMLab, which supports from data preparation to model deployment [6].Therefore, our system can manage large-scale datasets and can support endto-end ML lifecycle versions, from data to inference versions.

æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†ç³»ç»Ÿçš„ç¬¬ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸä¸­ç®¡ç†å¤§é‡ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå»ºç«‹åœ¨ä¸€ä¸ªå†…éƒ¨å†…å­˜å­˜å‚¨å¼•æ“çš„åŸºç¡€ä¸Šï¼Œç”¨äºå¤§è§„æ¨¡å­˜å‚¨ï¼Œé›†æˆäº†ä¸€ä¸ªä¼ä¸šå¼ºåº¦çš„å›¾å½¢æ•°æ®åº“ï¼Œå¦‚Neo4j[31]ï¼Œç”¨äºåŸºäºå›¾å½¢çš„ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ç®¡ç†ï¼Œä»¥åŠä¸€ä¸ªç»Ÿä¸€çš„æœºå™¨å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼ŒOpenMMLabï¼Œå®ƒæ”¯æŒä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²[6]ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥ç®¡ç†å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œå¹¶ä¸”å¯ä»¥æ”¯æŒç«¯åˆ°ç«¯çš„MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ï¼Œä»æ•°æ®åˆ°æ¨ç†ç‰ˆæœ¬ã€‚

The second challenge deals with the problem of how to leverage a large number of historic ML lifecycle versions to efficiently build an ML lifecycle for a new ML application. Especially, this challenge raises two research questions: How to save time and computation in building an ML pipeline for a new training dataset; and How to efficiently retrain for new unseen data during the ML lifecycle. We illustrate the huge cost of building an end-to-end ML lifecycle through the object detection problem, which is an important ML task for many real-world applications.

ç¬¬äºŒä¸ªæŒ‘æˆ˜æ¶‰åŠå¦‚ä½•åˆ©ç”¨å¤§é‡å†å²MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬æ¥æœ‰æ•ˆåœ°ä¸ºæ–°çš„MLåº”ç”¨ç¨‹åºæ„å»ºMLç”Ÿå‘½å‘¨æœŸçš„é—®é¢˜ã€‚  

ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸€æŒ‘æˆ˜æå‡ºäº†ä¸¤ä¸ªç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ºæ–°çš„è®­ç»ƒæ•°æ®é›†æ„å»ºæœºå™¨å­¦ä¹ ç®¡é“æ—¶èŠ‚çœæ—¶é—´å’Œè®¡ç®—ï¼›ä»¥åŠå¦‚ä½•åœ¨æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸä¸­æœ‰æ•ˆåœ°é‡æ–°è®­ç»ƒæ–°çš„æœªè§è¿‡çš„æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡å¯¹è±¡æ£€æµ‹é—®é¢˜è¯´æ˜äº†æ„å»ºç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸçš„å·¨å¤§æˆæœ¬ï¼Œè¿™æ˜¯è®¸å¤šç°å®ä¸–ç•Œåº”ç”¨ç¨‹åºçš„é‡è¦æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚

The training dataset for an object detection problem is often in large-scale. For example, the detection COCO [27] dataset has more than 120K data samples with the data size is 21GB. The BDD100K [40] dataset for diverse driving has 100K object detection frames. To build an ML pipeline for a training data (e.g., COCO dataset), an ML engineer will need to try with many data transformation techniques, ML model algorithms, training hyper-parameters, and inference configurations to achieve the final target (e.g., the highest testing accuracy). We experimented with 4 Nvidia Titan GPUs, each with 24GB GPU memory, then the training time for just one ML pipeline is around 12 hours. The ML engineer can use some automated ML algorithms such as NAS-FCOS [43] to automatically find an ML pipeline, but the search cost for a training data is very high, 28 GPU-days, which is inefficient in production.

ç›®æ ‡æ£€æµ‹é—®é¢˜çš„è®­ç»ƒæ•°æ®é›†é€šå¸¸æ˜¯å¤§è§„æ¨¡çš„ã€‚ä¾‹å¦‚ï¼Œæ£€æµ‹COCO[27]æ•°æ®é›†æœ‰120Kå¤šä¸ªæ•°æ®æ ·æœ¬ï¼Œæ•°æ®å¤§å°ä¸º21GBã€‚ç”¨äºå¤šç§é©¾é©¶çš„BDD100K[40]æ•°æ®é›†å…·æœ‰100Kä¸ªç›®æ ‡æ£€æµ‹å¸§ã€‚ä¸ºäº†ä¸ºè®­ç»ƒæ•°æ®(ä¾‹å¦‚COCOæ•°æ®é›†)æ„å»ºæœºå™¨å­¦ä¹ ç®¡é“ï¼Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆéœ€è¦å°è¯•è®¸å¤šæ•°æ®è½¬æ¢æŠ€æœ¯ã€æœºå™¨å­¦ä¹ æ¨¡å‹ç®—æ³•ã€è®­ç»ƒè¶…å‚æ•°å’Œæ¨ç†é…ç½®ï¼Œä»¥å®ç°æœ€ç»ˆç›®æ ‡(ä¾‹å¦‚ï¼Œæœ€é«˜çš„æµ‹è¯•ç²¾åº¦)ã€‚æˆ‘ä»¬ä½¿ç”¨4ä¸ªNvidia Titan GPUè¿›è¡Œå®éªŒï¼Œæ¯ä¸ªGPUå†…å­˜ä¸º24GBï¼Œé‚£ä¹ˆä»…ä¸€ä¸ªMLç®¡é“çš„è®­ç»ƒæ—¶é—´å°±åœ¨12å°æ—¶å·¦å³ã€‚æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå¯ä»¥ä½¿ç”¨ä¸€äº›è‡ªåŠ¨åŒ–çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚NAS-FCOS[43]æ¥è‡ªåŠ¨æŸ¥æ‰¾æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼Œä½†æ˜¯ä¸€ä¸ªè®­ç»ƒæ•°æ®çš„æœç´¢æˆæœ¬éå¸¸é«˜ï¼Œéœ€è¦28ä¸ªgpuå¤©ï¼Œåœ¨ç”Ÿäº§ä¸­æ•ˆç‡å¾ˆä½ã€‚


Another case is the requirement to rebuild an ML lifecycle when the ML data continuously evolves when the ML problem runs in real-world. This situation is very common for object detection tasks in real-life applications like selfdriving car where the autonomous car must deal with new driving cases throughout its lifetime. Therefore, it raises a crucial research question about building a lifecycle for an ML problem: How can we leverage our VeML system to effectively and efficiently build an end-to-end ML lifecycle for (1) a new training dataset and (2) new testing data during the ML lifetime?

å¦ä¸€ç§æƒ…å†µæ˜¯ï¼Œå½“æœºå™¨å­¦ä¹ æ•°æ®åœ¨ç°å®ä¸–ç•Œä¸­ä¸æ–­å‘å±•æ—¶ï¼Œéœ€è¦é‡æ–°æ„å»ºæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚è¿™ç§æƒ…å†µåœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ç­‰ç°å®åº”ç”¨ä¸­çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­éå¸¸å¸¸è§ï¼Œè‡ªåŠ¨é©¾é©¶æ±½è½¦å¿…é¡»åœ¨å…¶æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­å¤„ç†æ–°çš„é©¾é©¶æƒ…å†µã€‚å› æ­¤ï¼Œå®ƒæå‡ºäº†ä¸€ä¸ªå…³äºä¸ºæœºå™¨å­¦ä¹ é—®é¢˜æ„å»ºç”Ÿå‘½å‘¨æœŸçš„å…³é”®ç ”ç©¶é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨æˆ‘ä»¬çš„VeMLç³»ç»Ÿåœ¨æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸä¸­æœ‰æ•ˆå’Œé«˜æ•ˆåœ°ä¸º(1)æ–°çš„è®­ç»ƒæ•°æ®é›†å’Œ(2)æ–°çš„æµ‹è¯•æ•°æ®æ„å»ºç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸ?

End-to-end ML lifecycle for a training dataset A training dataset will start a ML pipeline for a new ML problem. To quickly build a lifecycle for the ML problem, we propose the lifecycle transferring algorithm, which uses the dataset similarity to transfer lifecycle versions of similar datasets. Our solution is inspired by transfer learning methodology in which we can transfer the whole ML pipeline to a similar dataset to save training time but still get high performance.

è®­ç»ƒæ•°æ®é›†å°†ä¸ºä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ é—®é¢˜å¯åŠ¨ä¸€ä¸ªæœºå™¨å­¦ä¹ ç®¡é“ã€‚ä¸ºäº†å¿«é€Ÿæ„å»ºMLé—®é¢˜çš„ç”Ÿå‘½å‘¨æœŸï¼Œæˆ‘ä»¬æå‡ºäº†ç”Ÿå‘½å‘¨æœŸè½¬ç§»ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨æ•°æ®é›†ç›¸ä¼¼åº¦æ¥è½¬ç§»ç›¸ä¼¼æ•°æ®é›†çš„ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå—åˆ°è¿ç§»å­¦ä¹ æ–¹æ³•çš„å¯å‘ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•´ä¸ªMLç®¡é“è½¬ç§»åˆ°ç±»ä¼¼çš„æ•°æ®é›†ä¸Šï¼Œä»¥èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä½†ä»ç„¶å¯ä»¥è·å¾—é«˜æ€§èƒ½ã€‚


The challenge is to efficiently compute dataset similarity for large-scale, high-dimensional data. ML datasets are often high dimensions (e.g., 1280x720 image data) and consist of large samples (e.g., COCO, BDD datasets have more than 100K examples). Thus, it is very inefficient to compute dataset similarity using all data samples of each dataset. To solve it, we propose representing each dataset as a small core set that can cover its distribution to efficiently compute similarity for each pair of datasets in the VeML system.

æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°è®¡ç®—å¤§è§„æ¨¡ã€é«˜ç»´æ•°æ®çš„æ•°æ®é›†ç›¸ä¼¼åº¦ã€‚æœºå™¨å­¦ä¹ æ•°æ®é›†é€šå¸¸æ˜¯é«˜ç»´çš„(ä¾‹å¦‚ï¼Œ1280x720å›¾åƒæ•°æ®)ï¼Œå¹¶ä¸”ç”±å¤§æ ·æœ¬ç»„æˆ(ä¾‹å¦‚ï¼ŒCOCO, BDDæ•°æ®é›†æœ‰è¶…è¿‡10ä¸‡ä¸ªæ ·æœ¬)ã€‚å› æ­¤ï¼Œä½¿ç”¨æ¯ä¸ªæ•°æ®é›†çš„æ‰€æœ‰æ•°æ®æ ·æœ¬æ¥è®¡ç®—æ•°æ®é›†ç›¸ä¼¼åº¦æ˜¯éå¸¸ä½æ•ˆçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†æ¯ä¸ªæ•°æ®é›†è¡¨ç¤ºä¸ºä¸€ä¸ªå¯ä»¥è¦†ç›–å…¶åˆ†å¸ƒçš„å°æ ¸å¿ƒé›†ï¼Œä»¥æœ‰æ•ˆåœ°è®¡ç®—VeMLç³»ç»Ÿä¸­æ¯å¯¹æ•°æ®é›†çš„ç›¸ä¼¼åº¦ã€‚

End-to-end ML lifecycle for new testing data A new testing data is a collection of unseen data samples when the ML problem runs in the real-world production. As a result, new testing data continuously come during the ML lifetime. A drift testing data is a data version that causes the (deployed) model accuracy significantly drops. The drift testing data version is derived from a different distribution than the training data version. If the testing and training data version are drawn from the same data distribution, no model accuracy degradation occurs; thus, the ML lifecycle remains. On the other hand, retraining is needed, then we need to construct a new ML lifecycle for the new testing data version.

æ–°æµ‹è¯•æ•°æ®çš„ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸå½“æœºå™¨å­¦ä¹ é—®é¢˜åœ¨å®é™…ç”Ÿäº§ä¸­è¿è¡Œæ—¶ï¼Œæ–°çš„æµ‹è¯•æ•°æ®æ˜¯ä¸€ç»„çœ‹ä¸è§çš„æ•°æ®æ ·æœ¬ã€‚å› æ­¤ï¼Œåœ¨æœºå™¨å­¦ä¹ çš„ç”Ÿå‘½å‘¨æœŸä¸­ï¼Œæ–°çš„æµ‹è¯•æ•°æ®ä¸æ–­å‡ºç°ã€‚æ¼‚ç§»æµ‹è¯•æ•°æ®æ˜¯å¯¼è‡´(éƒ¨ç½²çš„)æ¨¡å‹ç²¾åº¦æ˜¾è‘—ä¸‹é™çš„æ•°æ®ç‰ˆæœ¬ã€‚æ¼‚ç§»æµ‹è¯•æ•°æ®ç‰ˆæœ¬æ¥è‡ªä¸è®­ç»ƒæ•°æ®ç‰ˆæœ¬ä¸åŒçš„åˆ†å¸ƒã€‚å¦‚æœæµ‹è¯•æ•°æ®ç‰ˆæœ¬å’Œè®­ç»ƒæ•°æ®ç‰ˆæœ¬æ¥è‡ªç›¸åŒçš„æ•°æ®åˆ†å¸ƒï¼Œåˆ™ä¸ä¼šå‘ç”Ÿæ¨¡å‹ç²¾åº¦ä¸‹é™;å› æ­¤ï¼ŒMLç”Ÿå‘½å‘¨æœŸä»ç„¶å­˜åœ¨ã€‚å¦ä¸€æ–¹é¢ï¼Œéœ€è¦é‡æ–°è®­ç»ƒï¼Œç„¶åæˆ‘ä»¬éœ€è¦ä¸ºæ–°çš„æµ‹è¯•æ•°æ®ç‰ˆæœ¬æ„å»ºæ–°çš„MLç”Ÿå‘½å‘¨æœŸã€‚

In this paper, we propose to compare the core set of both testing and training data versions to detect data distribution mismatch without getting labeled test data, which is human cost saving. The next challenge is how to efficiently rebuild an ML lifecycle for a new testing data version in the case of the data distribution difference. We achieve this by allowing ML engineers to choose from various incremental training methods and VeML will automatically rebuild a new ML lifecycle after that.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æ¯”è¾ƒæµ‹è¯•å’Œè®­ç»ƒæ•°æ®ç‰ˆæœ¬çš„æ ¸å¿ƒé›†æ¥æ£€æµ‹æ•°æ®åˆ†å¸ƒä¸åŒ¹é…ï¼Œè€Œä¸éœ€è¦æ ‡è®°æµ‹è¯•æ•°æ®ï¼Œä»è€ŒèŠ‚çœäººåŠ›æˆæœ¬ã€‚ä¸‹ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨æ•°æ®åˆ†å¸ƒå·®å¼‚çš„æƒ…å†µä¸‹ä¸ºæ–°çš„æµ‹è¯•æ•°æ®ç‰ˆæœ¬æœ‰æ•ˆåœ°é‡å»ºMLç”Ÿå‘½å‘¨æœŸã€‚æˆ‘ä»¬é€šè¿‡å…è®¸æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä»å„ç§å¢é‡è®­ç»ƒæ–¹æ³•ä¸­è¿›è¡Œé€‰æ‹©æ¥å®ç°è¿™ä¸€ç‚¹ï¼ŒVeMLå°†åœ¨æ­¤ä¹‹åè‡ªåŠ¨é‡å»ºæ–°çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚

In summary, we present our contributions for this research as follows:
ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬åœ¨æœ¬ç ”ç©¶ä¸­çš„è´¡çŒ®å¦‚ä¸‹: 

â€¢ We build a version management system dedicated to end-to-end ML lifecycle (VeML), from data to inference. Our system implements numerous functionalities to help manage huge ML lifecycle versions.
æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿï¼Œè‡´åŠ›äºç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸ(VeML)ï¼Œä»æ•°æ®åˆ°æ¨ç†ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå®ç°äº†è®¸å¤šåŠŸèƒ½æ¥å¸®åŠ©ç®¡ç†å·¨å¤§çš„MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ã€‚

â€¢ We propose an algorithm based on the core set to efficient comparing large-scale and high-dimensional data versions. We prove our solution on large-scale driving images and spatiotemporal sensor datasets.
æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ ¸å¿ƒé›†çš„ç®—æ³•ï¼Œä»¥æœ‰æ•ˆåœ°æ¯”è¾ƒå¤§è§„æ¨¡å’Œé«˜ç»´æ•°æ®ç‰ˆæœ¬ã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡é©¾é©¶å›¾åƒå’Œæ—¶ç©ºä¼ æ„Ÿå™¨æ•°æ®é›†ä¸Šè¯æ˜äº†æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆã€‚

â€¢ Using dataset similarity computation, our system can transfer lifecycle versions of similar datasets to effectively and efficiently build an ML lifecycle for a new ML problem.
ä½¿ç”¨æ•°æ®é›†ç›¸ä¼¼åº¦è®¡ç®—ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥è½¬ç§»ç±»ä¼¼æ•°æ®é›†çš„ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¸ºæ–°çš„æœºå™¨å­¦ä¹ é—®é¢˜æ„å»ºæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚

â€¢ We employ the core set computation to detect data distributions dissimilarity between the testing and training data versions without getting labeled data. Based on the unsupervised data distribution mismatch detection, VeML can support automatically rebuild a ML lifecycle after choosing a model retraining method.
æˆ‘ä»¬ä½¿ç”¨æ ¸å¿ƒé›†è®¡ç®—æ¥æ£€æµ‹æµ‹è¯•å’Œè®­ç»ƒæ•°æ®ç‰ˆæœ¬ä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼Œè€Œä¸éœ€è¦æ ‡è®°æ•°æ®ã€‚åŸºäºæ— ç›‘ç£æ•°æ®åˆ†å¸ƒä¸åŒ¹é…æ£€æµ‹ï¼ŒVeMLæ”¯æŒåœ¨é€‰æ‹©æ¨¡å‹å†è®­ç»ƒæ–¹æ³•åè‡ªåŠ¨é‡å»ºæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚

â€¢ Moreover, to demonstrate that our system is helpful, we show how VeML is using in an on-going self-driving project and how it supports new challenges in ML lifecycle.
æ­¤å¤–ï¼Œä¸ºäº†è¯æ˜æˆ‘ä»¬çš„ç³»ç»Ÿæ˜¯æœ‰ç”¨çš„ï¼Œæˆ‘ä»¬å±•ç¤ºäº†VeMLå¦‚ä½•åœ¨æ­£åœ¨è¿›è¡Œçš„è‡ªåŠ¨é©¾é©¶é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œä»¥åŠå®ƒå¦‚ä½•æ”¯æŒæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸä¸­çš„æ–°æŒ‘æˆ˜ã€‚


The rest of this paper is structured as follows.
æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»“æ„å¦‚ä¸‹ã€‚

Section 2 presents related research to our work.
ç¬¬2èŠ‚ä»‹ç»äº†æˆ‘ä»¬å·¥ä½œçš„ç›¸å…³ç ”ç©¶ã€‚

Section 3 describes our system architecture and functionalities in detail.
ç¬¬3èŠ‚è¯¦ç»†æè¿°äº†æˆ‘ä»¬çš„ç³»ç»Ÿæ¶æ„å’ŒåŠŸèƒ½ã€‚

Section 4 presents how to transfer ML lifecycle versions for a new training dataset.
ç¬¬4èŠ‚ä»‹ç»äº†å¦‚ä½•ä¸ºæ–°çš„è®­ç»ƒæ•°æ®é›†è½¬ç§»MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ã€‚

Next, section 5 shows how to detect data distribution mismatch and rebuild a new ML lifecycle.
æ¥ä¸‹æ¥ï¼Œç¬¬5èŠ‚å°†å±•ç¤ºå¦‚ä½•æ£€æµ‹æ•°æ®åˆ†å¸ƒä¸åŒ¹é…å¹¶é‡æ–°æ„å»ºæ–°çš„MLç”Ÿå‘½å‘¨æœŸã€‚

Then, section 6 demonstrates the usefulness of our VeML system.
ç„¶åï¼Œç¬¬6èŠ‚å°†æ¼”ç¤ºæˆ‘ä»¬çš„VeMLç³»ç»Ÿçš„æœ‰ç”¨æ€§ã€‚

And finally, section 7 wraps up our contributions and discusses future work.
æœ€åï¼Œç¬¬7èŠ‚æ€»ç»“äº†æˆ‘ä»¬çš„è´¡çŒ®å¹¶è®¨è®ºäº†æœªæ¥çš„å·¥ä½œã€‚

## II.RELATED WORK

This section discusses related research in ML lifecycle platforms, version control systems, and ML automation which directly connects to our research.        We also survey papers tackling data-related challenges, such as dataset similarity, data drift detection, and incremental training with new data.

æœ¬èŠ‚è®¨è®ºä¸æˆ‘ä»¬çš„ç ”ç©¶ç›´æ¥ç›¸å…³çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸå¹³å°ã€ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå’Œæœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–æ–¹é¢çš„ç›¸å…³ç ”ç©¶ã€‚æˆ‘ä»¬è¿˜è°ƒæŸ¥äº†å¤„ç†æ•°æ®ç›¸å…³æŒ‘æˆ˜çš„è®ºæ–‡ï¼Œå¦‚æ•°æ®é›†ç›¸ä¼¼æ€§ã€æ•°æ®æ¼‚ç§»æ£€æµ‹å’Œæ–°æ•°æ®çš„å¢é‡è®­ç»ƒã€‚

ML Lifecycle Platforms Many ML lifecycle platforms have been proposed to support ML tasks in production.        One of the first such systems is Google Tensorflow Extended (TFX) [20], which has been introduced since 2017.        TFX is a TensorFlow-based ML platform, from data preparation to model training and production serving.        The versioning information is managed by a metadata tool and can be saved to a database like SQLite or MySQL.        MLFlow [41] was presented by DataBricks, the company behind the large-scale data analysis Apache Spark, in 2018.        MLFlow is an open-source platform that supports packaging and tracking ML experiments runs and reproducing.        It manages ML experiment versions in artifact concepts, such as data files, models, and training codes.        Data platform for ML (MLdp) [1] was introduced as an internal platform by Apple in 2019.        It has an integrated data system, supports data versioning, and integrates with inhouse solutions for model training and deployment.

è®¸å¤šæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸå¹³å°è¢«æå‡ºæ¥æ”¯æŒç”Ÿäº§ä¸­çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚é¦–æ‰¹è¿™æ ·çš„ç³»ç»Ÿä¹‹ä¸€æ˜¯Google Tensorflow Extended (TFX)[20]ï¼Œè¯¥ç³»ç»Ÿäº2017å¹´æ¨å‡ºã€‚TFXæ˜¯ä¸€ä¸ªåŸºäºtensorflowçš„æœºå™¨å­¦ä¹ å¹³å°ï¼Œä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹åŸ¹è®­å’Œç”Ÿäº§æœåŠ¡ã€‚ç‰ˆæœ¬æ§åˆ¶ä¿¡æ¯ç”±å…ƒæ•°æ®å·¥å…·ç®¡ç†ï¼Œå¯ä»¥ä¿å­˜åˆ°SQLiteæˆ–MySQLç­‰æ•°æ®åº“ä¸­ã€‚MLFlow[41]äº2018å¹´ç”±DataBricks(å¤§å‹æ•°æ®åˆ†æApache SparkèƒŒåçš„å…¬å¸)æå‡ºã€‚MLFlowæ˜¯ä¸€ä¸ªæ”¯æŒæ‰“åŒ…å’Œè·Ÿè¸ªMLå®éªŒè¿è¡Œå’Œé‡ç°çš„å¼€æºå¹³å°ã€‚å®ƒç®¡ç†å·¥ä»¶æ¦‚å¿µä¸­çš„MLå®éªŒç‰ˆæœ¬ï¼Œä¾‹å¦‚æ•°æ®æ–‡ä»¶ã€æ¨¡å‹å’Œè®­ç»ƒä»£ç ã€‚MLæ•°æ®å¹³å°(Data platform for ML, MLdp)[1]æ˜¯è‹¹æœå…¬å¸åœ¨2019å¹´æ¨å‡ºçš„å†…éƒ¨å¹³å°ã€‚å®ƒæœ‰ä¸€ä¸ªé›†æˆçš„æ•°æ®ç³»ç»Ÿï¼Œæ”¯æŒæ•°æ®ç‰ˆæœ¬æ§åˆ¶ï¼Œå¹¶é›†æˆäº†ç”¨äºæ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„å†…éƒ¨è§£å†³æ–¹æ¡ˆã€‚

æˆ‘ä»¬æ˜¯å¦é¦–å…ˆæ”¯æŒtfxï¼Ÿ éœ€è¦å…ˆæ‰¾åˆ°å®¢æˆ·ã€‚

TFX: A TensorFlow-Based Production-Scale Machine Learning Platform
https://dl.acm.org/doi/pdf/10.1145/3097983.3098021

tensorflow/tensorflow: An Open Source Machine Learning Framework for Everyone
https://github.com/tensorflow/tensorflow

arXiv - Towards ML Engineering - A Brief History Of TensorFlow Extended (TFX)
https://arxiv.org/pdf/2010.02013.pdf

Continuous Training and Deployment of Deep Learning Models | Datenbank-Spektrum
https://link.springer.com/article/10.1007/s13222-021-00386-8

Arangopipe, a tool for machine learning meta-data management - IOS Press
https://content.iospress.com/articles/data-science/ds210034

Arangopipe, a tool for machine learning meta-data management
https://content.iospress.com/download/data-science/ds210034?id=data-science%2Fds210034

arangodb/arangodb: ğŸ¥‘ ArangoDB is a native multi-model database with flexible data models for documents, graphs, and key-values. Build high performance applications using a convenient SQL-like query language or JavaScript extensions.
https://github.com/arangodb/arangodb

Efficient ML Lifecycle Transferring for Large-Scale and High-Dimensional Data via Core Set-Based Dataset Similarity | IEEE Journals & Magazine | IEEE Xplore
https://ieeexplore.ieee.org/document/10185033

IEEE Xplore Full-Text PDF:
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10185033

arXiv - Towards ML Engineering - A Brief History Of TensorFlow Extended (TFX)
https://arxiv.org/pdf/2010.02013.pdf

Collaborative Machine Learning Model Building with Families Using Co-ML
https://arxiv.org/pdf/2304.05444.pdf

Dataset and Network Introspection ToolKit (DNIKit) - Apple Machine Learning Research
https://machinelearning.apple.com/research/dnikit

Apple
https://github.com/apple?q=&type=all&language=&sort=

Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale
https://arxiv.org/pdf/2204.07309.pdf

1909.05372.pdf
https://arxiv.org/pdf/1909.05372.pdf

Data Platform for Machine Learning - Apple Machine Learning Research
https://machinelearning.apple.com/research/data-platform-machine-learning

Data Platform for Machine Learning
https://dl.acm.org/doi/pdf/10.1145/3299869.3314050

Architectural Components in ML-Enabled Systems | by Christian KÃ¤stner | Medium
https://ckaestne.medium.com/architectural-components-in-ml-enabled-systems-78cf76b29a92

Christian KÃ¤stner â€“ Medium
https://ckaestne.medium.com/

Security and Privacy in ML-Enabled Systems | by Christian KÃ¤stner | Medium
https://ckaestne.medium.com/security-and-privacy-in-ml-enabled-systems-1855f561b894

ç”Ÿäº§ä¸­çš„æœºå™¨å­¦ä¹ ï¼šä»æ¨¡å‹åˆ°äº§å“ | ä½œè€…ï¼šå…‹é‡Œæ–¯è’‚å®‰Â·å¡æ–¯ç‰¹çº³ ä¸­ç­‰çš„
https://ckaestne.medium.com/machine-learning-in-production-book-overview-63be62393581

Machine Learning in Production / AI Engineering
https://ckaestne.github.io/seai/

``Everyone wants to do the model work, not the data work'': Data Cascades in High-Stakes AI
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/0d556e45afc54afeb2eb6b51a9bc1827b9961ff4.pdf

Collaboration Challenges in Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process
https://arxiv.org/pdf/2110.10234.pdf

2105.12422.pdf
https://arxiv.org/pdf/2105.12422.pdf

Actionable Data Insights for Machine Learning
https://dl.acm.org/doi/pdf/10.1145/3578356.3592581

ieee_mlflow.pdf
https://people.eecs.berkeley.edu/~matei/papers/2018/ieee_mlflow.pdf

Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle
https://people.eecs.berkeley.edu/~matei/papers/2020/deem_mlflow.pdf

In general, these ML lifecycle platforms do not have endto-end ML lifecycle version management, from data to inference.        In the case of TFX, it supports end-to-end ML lifecycle but does not help build a new ML lifecycle employing managed lifecycle versions as our system.

ä¸€èˆ¬æ¥è¯´ï¼Œè¿™äº›æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸå¹³å°æ²¡æœ‰ç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ç®¡ç†ï¼Œä»æ•°æ®åˆ°æ¨ç†ã€‚åœ¨TFXçš„æƒ…å†µä¸‹ï¼Œå®ƒæ”¯æŒç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸï¼Œä½†ä¸å¸®åŠ©æ„å»ºä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸï¼Œä½¿ç”¨ç®¡ç†ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ä½œä¸ºæˆ‘ä»¬çš„ç³»ç»Ÿã€‚

```æˆ‘æƒ³è¿™é‡Œæ˜¯æˆ‘ä»¬çš„æœºä¼š ```

Recently, MLOps for end-to-end ML lifecycle are emerging.        They are provided by many big companies such as Google Cloud [21], Amazon Sagemaker [3], and Microsoft Azure [4].        These systems support data scientists building endto-end ML problems, from data to deployment, but still do not leverage many lifecycle versions to quickly construct a lifecycle for an ML problem.

æœ€è¿‘ï¼Œç«¯åˆ°ç«¯MLç”Ÿå‘½å‘¨æœŸçš„mlopæ­£åœ¨å‡ºç°ã€‚å®ƒä»¬ç”±è®¸å¤šå¤§å…¬å¸æä¾›ï¼Œå¦‚Google Cloud[21]ã€Amazon Sagemaker[3]å’ŒMicrosoft Azure[4]ã€‚è¿™äº›ç³»ç»Ÿæ”¯æŒæ•°æ®ç§‘å­¦å®¶æ„å»ºç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œä»æ•°æ®åˆ°éƒ¨ç½²ï¼Œä½†ä»ç„¶æ²¡æœ‰åˆ©ç”¨è®¸å¤šç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬æ¥å¿«é€Ÿæ„å»ºæœºå™¨å­¦ä¹ é—®é¢˜çš„ç”Ÿå‘½å‘¨æœŸã€‚

2111.13657.pdf
https://arxiv.org/pdf/2111.13657.pdf

Amazon SageMaker Debugger: A System for Real-Time Insights into Machine Learning Model Training
https://assets.amazon.science/0b/cb/47bb9a1e4b6a8f78ed7a7611f4a7/amazon-sagemaker-debugger-a-system-for-real-time-insights-into-machine-learning-model-training.pdf

Amazon SageMaker Autopilot: a white box AutoML solution at scale
https://arxiv.org/pdf/2012.08483.pdf

Amazon SageMaker Model Parallelism: A General and Flexible Framework for Large Model Training
https://arxiv.org/pdf/2111.05972.pdf

Elastic Machine Learning Algorithms in Amazon SageMaker
https://edoliberty.github.io/papers/sagemaker.pdf

Version Management for ML With the increasing importance of ML versioning management, many solutions have been introduced for ML version control, especially for data versions.        Typically, datasets for ML tasks are stored in file systems, causing managing many versions of them difficult and inefficient.

MLçš„ç‰ˆæœ¬ç®¡ç†éšç€MLç‰ˆæœ¬ç®¡ç†çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ï¼Œå·²ç»ä¸ºMLç‰ˆæœ¬æ§åˆ¶å¼•å…¥äº†è®¸å¤šè§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯æ•°æ®ç‰ˆæœ¬æ§åˆ¶ã€‚é€šå¸¸ï¼ŒMLä»»åŠ¡çš„æ•°æ®é›†å­˜å‚¨åœ¨æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œè¿™å¯¼è‡´ç®¡ç†å¤šä¸ªç‰ˆæœ¬çš„æ•°æ®é›†å˜å¾—å›°éš¾ä¸”æ•ˆç‡ä½ä¸‹ã€‚

Paper [18] proposed to build a data version management system over a relational database.        Their solution was to separate the data from the version information in two tables.        The data table stores the records are appearing in any data version, while the version table captures the versioning information of which version contains which records.        They presented the partitioning optimization problem, given a version-record bipartite graph, minimizing the checkout and storage cost, which is an NP-hard problem.

æ–‡çŒ®[18]æå‡ºåœ¨å…³ç³»æ•°æ®åº“ä¸Šæ„å»ºæ•°æ®ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿã€‚ä»–ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨ä¸¤ä¸ªè¡¨ä¸­å°†æ•°æ®ä¸ç‰ˆæœ¬ä¿¡æ¯åˆ†å¼€ã€‚æ•°æ®è¡¨å­˜å‚¨å‡ºç°åœ¨ä»»ä½•æ•°æ®ç‰ˆæœ¬ä¸­çš„è®°å½•ï¼Œè€Œç‰ˆæœ¬è¡¨æ•è·å“ªä¸ªç‰ˆæœ¬åŒ…å«å“ªäº›è®°å½•çš„ç‰ˆæœ¬æ§åˆ¶ä¿¡æ¯ã€‚ä»–ä»¬æå‡ºäº†åˆ†åŒºä¼˜åŒ–é—®é¢˜ï¼Œç»™å®šä¸€ä¸ªç‰ˆæœ¬è®°å½•äºŒéƒ¨å›¾ï¼Œæœ€å°åŒ–ç»“å¸å’Œå­˜å‚¨æˆæœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªnpå›°éš¾é—®é¢˜ã€‚

1703.02475.pdf
https://arxiv.org/pdf/1703.02475.pdf

orpheus.pdf
https://people.eecs.berkeley.edu/~adityagp/papers/orpheus.pdf

Our data version management also bases on this idea by separating the data and version storage.        We save data samples into in-memory storage but manage the version information in a graph database.        Our solution may not optimize the storage cost, but it helps us to load any data versions constantly, which is critical for reproducing any ML training processes during an ML lifecycle.

æˆ‘ä»¬çš„æ•°æ®ç‰ˆæœ¬ç®¡ç†ä¹ŸåŸºäºè¿™ä¸ªæƒ³æ³•ï¼Œå°†æ•°æ®å’Œç‰ˆæœ¬å­˜å‚¨åˆ†å¼€ã€‚æˆ‘ä»¬å°†æ•°æ®æ ·æœ¬ä¿å­˜åˆ°å†…å­˜å­˜å‚¨ä¸­ï¼Œä½†åœ¨å›¾å½¢æ•°æ®åº“ä¸­ç®¡ç†ç‰ˆæœ¬ä¿¡æ¯ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¯èƒ½æ— æ³•ä¼˜åŒ–å­˜å‚¨æˆæœ¬ï¼Œä½†å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸æ–­åŠ è½½ä»»ä½•æ•°æ®ç‰ˆæœ¬ï¼Œè¿™å¯¹äºåœ¨æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸä¸­é‡ç°ä»»ä½•æœºå™¨å­¦ä¹ è®­ç»ƒè¿‡ç¨‹è‡³å…³é‡è¦ã€‚

Moreover, many systems and tools have been proposed to manage data and model versions for the ML lifecycle.        For instance, Data Version Control (DVC) [19] is a popular opensource tool.        DVC lets us capture versions of data and models in Git commits while storing them on-premises or in the cloud.        However, no systems supports us in managing end-toend ML lifecycle versions and leveraging managed versions to build a new ML lifecycle.

æ­¤å¤–ï¼Œå·²ç»æå‡ºäº†è®¸å¤šç³»ç»Ÿå’Œå·¥å…·æ¥ç®¡ç†MLç”Ÿå‘½å‘¨æœŸçš„æ•°æ®å’Œæ¨¡å‹ç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼Œæ•°æ®ç‰ˆæœ¬æ§åˆ¶(DVC)[19]æ˜¯ä¸€ä¸ªæµè¡Œçš„å¼€æºå·¥å…·ã€‚DVCå…è®¸æˆ‘ä»¬åœ¨Gitæäº¤ä¸­æ•è·æ•°æ®å’Œæ¨¡å‹çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶å°†å®ƒä»¬å­˜å‚¨åœ¨æœ¬åœ°æˆ–äº‘ä¸­ã€‚ç„¶è€Œï¼Œæ²¡æœ‰ç³»ç»Ÿæ”¯æŒæˆ‘ä»¬ç®¡ç†ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ï¼Œå¹¶åˆ©ç”¨å·²ç®¡ç†çš„ç‰ˆæœ¬æ¥æ„å»ºæ–°çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚

ML Automation There are a number of systems that serves automatic searching for the best ML model such as AutoML systems for ML [12], [16], [32] or NAS systems [42] for deep learning (DL) problems.      These systems search for ML/DL pipelines from a set of predefined ML/DL operators and then execute experiments with many training hyperparameter combinations.      They also leverage similar datasets as a meta-learning approach for more efficient ML pipeline exploration [12], [16].

æœ‰è®¸å¤šç³»ç»Ÿæä¾›è‡ªåŠ¨æœç´¢æœ€ä½³MLæ¨¡å‹çš„æœåŠ¡ï¼Œä¾‹å¦‚ç”¨äºML[12]ã€[16]ã€[32]çš„AutoMLç³»ç»Ÿæˆ–ç”¨äºæ·±åº¦å­¦ä¹ (DL)é—®é¢˜çš„NASç³»ç»Ÿ[42]ã€‚è¿™äº›ç³»ç»Ÿä»ä¸€ç»„é¢„å®šä¹‰çš„ML/DLæ“ä½œç¬¦ä¸­æœç´¢ML/DLç®¡é“ï¼Œç„¶åä½¿ç”¨è®¸å¤šè®­ç»ƒè¶…å‚æ•°ç»„åˆæ‰§è¡Œå®éªŒã€‚ä»–ä»¬è¿˜åˆ©ç”¨ç±»ä¼¼çš„æ•°æ®é›†ä½œä¸ºå…ƒå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ›´æœ‰æ•ˆçš„æœºå™¨å­¦ä¹ ç®¡é“æ¢ç´¢[12]ï¼Œ[16]ã€‚

The most dissimilarity of these systems to ours is that they search for an ML pipeline for each new dataset, which is time-consuming and high-cost.      On the other hand, our system leverages many ML lifecycle versions to effectively and efficiently build new lifecycle for training data and testing data versions.    

è¿™äº›ç³»ç»Ÿä¸æˆ‘ä»¬çš„ç³»ç»Ÿæœ€å¤§çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä»¬ä¸ºæ¯ä¸ªæ–°æ•°æ®é›†æœç´¢MLç®¡é“ï¼Œè¿™æ˜¯è€—æ—¶ä¸”é«˜æˆæœ¬çš„ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåˆ©ç”¨è®¸å¤šæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬æ¥æœ‰æ•ˆå’Œé«˜æ•ˆåœ°ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ç‰ˆæœ¬æ„å»ºæ–°çš„ç”Ÿå‘½å‘¨æœŸã€‚

Dataset Similarity
To compute dataset similarity, metafeatures based computation is one of the most popular solutions [12].   However, meta-features are often unavailable for high-dimensional data such as image or spatiotemporal data.   Using dataset embedding [15] for dataset similarity computation is also a common method, but it is inefficient when computing with a large number of data samples.

æ•°æ®é›†ç›¸ä¼¼
ä¸ºäº†è®¡ç®—æ•°æ®é›†ç›¸ä¼¼åº¦ï¼ŒåŸºäºå…ƒç‰¹å¾çš„è®¡ç®—æ˜¯æœ€æµè¡Œçš„è§£å†³æ–¹æ¡ˆä¹‹ä¸€[12]ã€‚ç„¶è€Œï¼Œå…ƒç‰¹å¾é€šå¸¸æ— æ³•ç”¨äºé«˜ç»´æ•°æ®ï¼Œå¦‚å›¾åƒæˆ–æ—¶ç©ºæ•°æ®ã€‚ä½¿ç”¨æ•°æ®é›†åµŒå…¥[15]è¿›è¡Œæ•°æ®é›†ç›¸ä¼¼åº¦è®¡ç®—ä¹Ÿæ˜¯ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•ï¼Œä½†åœ¨è®¡ç®—å¤§é‡æ•°æ®æ ·æœ¬æ—¶æ•ˆç‡ä½ä¸‹ã€‚

Another recent proposal is computing geometric dataset distances based on optimal transport [2].   This method worked for classification datasets but still suffered the high-cost problem when dealing with large-scale datasets.   Our similarity computation is based on the core set, a small subset of a dataset, and thus, possible to work with large-scale and highdimensional datasets.

æœ€è¿‘çš„å¦ä¸€ä¸ªå»ºè®®æ˜¯åŸºäºæœ€ä¼˜ä¼ è¾“è®¡ç®—å‡ ä½•æ•°æ®é›†è·ç¦»[2]ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºåˆ†ç±»æ•°æ®é›†ï¼Œä½†åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ä»ç„¶å­˜åœ¨é«˜æˆæœ¬é—®é¢˜ã€‚æˆ‘ä»¬çš„ç›¸ä¼¼æ€§è®¡ç®—åŸºäºæ ¸å¿ƒé›†ï¼Œå³æ•°æ®é›†çš„ä¸€ä¸ªå°å­é›†ï¼Œå› æ­¤å¯ä»¥å¤„ç†å¤§è§„æ¨¡å’Œé«˜ç»´æ•°æ®é›†

Data Drift Detection

æ•°æ®æ¼‚ç§»æ£€æµ‹

Detecting drift in the continuous data has been tackled in some papers [28], [37].   Matchmaker [28] uses a decision tree to detect both data drift and concept drift, but it only works well for tabular data.   ODIN [37] detects drift in video image data, but it still uses all data samples that may not be efficient for massive datasets.   Our solution is based on the small core set that can work for unlabeled continuing large-scale data versions.

ä¸€äº›è®ºæ–‡å·²ç»å¯¹è¿ç»­æ•°æ®çš„æ¼‚ç§»æ£€æµ‹è¿›è¡Œäº†ç ”ç©¶[28]ï¼Œ[37]ã€‚Matchmaker[28]ä½¿ç”¨å†³ç­–æ ‘æ¥æ£€æµ‹æ•°æ®æ¼‚ç§»å’Œæ¦‚å¿µæ¼‚ç§»ï¼Œä½†å®ƒåªé€‚ç”¨äºè¡¨æ ¼æ•°æ®ã€‚ODIN[37]æ£€æµ‹è§†é¢‘å›¾åƒæ•°æ®ä¸­çš„æ¼‚ç§»ï¼Œä½†å®ƒä»ç„¶ä½¿ç”¨æ‰€æœ‰çš„æ•°æ®æ ·æœ¬ï¼Œè¿™å¯¹äºæµ·é‡æ•°æ®é›†æ¥è¯´å¯èƒ½ä¸æ˜¯å¾ˆæœ‰æ•ˆã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŸºäºå°å‹æ ¸å¿ƒé›†ï¼Œå¯ä»¥ç”¨äºæœªæ ‡è®°çš„è¿ç»­å¤§è§„æ¨¡æ•°æ®ç‰ˆæœ¬ã€‚

Incremental Learning

å¢é‡å­¦ä¹ 

continuously retrain an ML model when a new training data comes.   Some popular model retraining methods are full training which retrains all available datasets, and transfer learning which only retrains the new dataset from a pre-trained model.   These approaches require labeling all available data samples, which is costly.   Other incremental learning algorithms, that reduce labeling cost, are active learning [35], [36], which tries to label a small number of the most significant training data, and domain adaptation [7], [22], [38], which learns from a source domain but can generalize to a different target domain without labeled data.

å½“æœ‰æ–°çš„è®­ç»ƒæ•°æ®å‡ºç°æ—¶ï¼Œä¸æ–­åœ°é‡æ–°è®­ç»ƒMLæ¨¡å‹ã€‚ä¸€äº›æµè¡Œçš„æ¨¡å‹å†è®­ç»ƒæ–¹æ³•æ˜¯å®Œå…¨è®­ç»ƒï¼Œå®ƒé‡æ–°è®­ç»ƒæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†ï¼Œä»¥åŠè¿ç§»å­¦ä¹ ï¼Œå®ƒåªä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­é‡æ–°è®­ç»ƒæ–°çš„æ•°æ®é›†ã€‚è¿™äº›æ–¹æ³•éœ€è¦æ ‡è®°æ‰€æœ‰å¯ç”¨çš„æ•°æ®æ ·æœ¬ï¼Œè¿™æ˜¯æ˜‚è´µçš„ã€‚å…¶ä»–å‡å°‘æ ‡è®°æˆæœ¬çš„å¢é‡å­¦ä¹ ç®—æ³•æœ‰ä¸»åŠ¨å­¦ä¹ [35]ï¼Œ[36]ï¼Œå®ƒè¯•å›¾æ ‡è®°å°‘é‡æœ€é‡è¦çš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠåŸŸé€‚åº”[7]ï¼Œ[22]ï¼Œ[38]ï¼Œå®ƒä»æºåŸŸå­¦ä¹ ï¼Œä½†å¯ä»¥åœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹æ¨å¹¿åˆ°ä¸åŒçš„ç›®æ ‡åŸŸã€‚

## III. SYSTEM ARCHITECTURE AND FUNCTIONALITIES ç³»ç»Ÿæ¶æ„å’ŒåŠŸèƒ½

#### A. SYSTEM ARCHITECTURE  ç³»ç»Ÿæ¶æ„

Our system architecture has three main blocks and other functional modules. The first is an in-memory storage engine built in our laboratory to manage large-scale data versions, training logs, and metadata information. The second is an integrated graph database such as Neo4j [31] for graph-based ML lifecycle version management and analysis. And the third component is an ML training framework which is built over the open-source OpenMMLab [6].

æˆ‘ä»¬çš„ç³»ç»Ÿæ¶æ„æœ‰ä¸‰ä¸ªä¸»è¦æ¨¡å—å’Œå…¶ä»–åŠŸèƒ½æ¨¡å—ã€‚ç¬¬ä¸€ä¸ªæ˜¯æˆ‘ä»¬å®éªŒå®¤æ„å»ºçš„å†…å­˜å­˜å‚¨å¼•æ“ï¼Œç”¨äºç®¡ç†å¤§è§„æ¨¡æ•°æ®ç‰ˆæœ¬ã€è®­ç»ƒæ—¥å¿—å’Œå…ƒæ•°æ®ä¿¡æ¯ã€‚ç¬¬äºŒç§æ˜¯é›†æˆçš„å›¾å½¢æ•°æ®åº“ï¼Œå¦‚Neo4j[31]ï¼Œç”¨äºåŸºäºå›¾å½¢çš„MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ç®¡ç†å’Œåˆ†æã€‚ç¬¬ä¸‰ä¸ªç»„ä»¶æ˜¯åŸºäºå¼€æºOpenMMLabæ„å»ºçš„MLè®­ç»ƒæ¡†æ¶[6]ã€‚

OpenMMLab is a unified architecture for many ML problems, integrating with common ML frameworks (like PyTorch [33]), easy to re-use and extend functions by a modular design. We leverage the OpenMMLab framework to perform ML training with training data from a data version, model algorithm configurations from a model version, and return trained model checkpoints for a training version. It also supports model deployment to an inference model running in ML serving systems. Figure 2 shows our system architecture with three main components and many functional modules. We use file systems to save binary objects like trained and deployed models.

OpenMMLabæ˜¯è§£å†³è®¸å¤šæœºå™¨å­¦ä¹ é—®é¢˜çš„ç»Ÿä¸€æ¶æ„ï¼Œä¸å¸¸è§çš„æœºå™¨å­¦ä¹ æ¡†æ¶(å¦‚PyTorch[33])é›†æˆï¼Œé€šè¿‡æ¨¡å—åŒ–è®¾è®¡æ˜“äºé‡ç”¨å’Œæ‰©å±•åŠŸèƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨OpenMMLabæ¡†æ¶ä½¿ç”¨æ¥è‡ªæ•°æ®ç‰ˆæœ¬çš„è®­ç»ƒæ•°æ®ã€æ¥è‡ªæ¨¡å‹ç‰ˆæœ¬çš„æ¨¡å‹ç®—æ³•é…ç½®æ‰§è¡ŒMLè®­ç»ƒï¼Œå¹¶è¿”å›è®­ç»ƒç‰ˆæœ¬çš„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚å®ƒè¿˜æ”¯æŒå°†æ¨¡å‹éƒ¨ç½²åˆ°åœ¨MLæœåŠ¡ç³»ç»Ÿä¸­è¿è¡Œçš„æ¨ç†æ¨¡å‹ã€‚å›¾2æ˜¾ç¤ºäº†æˆ‘ä»¬çš„ç³»ç»Ÿæ¶æ„ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶å’Œè®¸å¤šåŠŸèƒ½æ¨¡å—ã€‚æˆ‘ä»¬ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ¥ä¿å­˜äºŒè¿›åˆ¶å¯¹è±¡ï¼Œå¦‚è®­ç»ƒå’Œéƒ¨ç½²çš„æ¨¡å‹ã€‚


#### B. SYSTEM FUNCTIONALITIES ç³»ç»ŸåŠŸèƒ½

Firstly, we define how we manage the version of every component in the end-to-end ML lifecycle. A data version is a collection of data samples and its data preparation (e.g., normalization, missing values imputation). A training data version is a data version that is used as the training data for the ML task. A testing data version is a data version that contains the unseen new data collected from the real-world environment when an ML problem runs in production. The unseen test data will be annotated and routed back as training data when rebuilding the ML lifecycle.

é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº†å¦‚ä½•ç®¡ç†ç«¯åˆ°ç«¯MLç”Ÿå‘½å‘¨æœŸä¸­æ¯ä¸ªç»„ä»¶çš„ç‰ˆæœ¬ã€‚æ•°æ®ç‰ˆæœ¬æ˜¯æ•°æ®æ ·æœ¬åŠå…¶æ•°æ®å‡†å¤‡çš„é›†åˆ(ä¾‹å¦‚ï¼Œå½’ä¸€åŒ–ï¼Œç¼ºå¤±å€¼è¾“å…¥)ã€‚è®­ç»ƒæ•°æ®ç‰ˆæœ¬æ˜¯ç”¨ä½œMLä»»åŠ¡çš„è®­ç»ƒæ•°æ®çš„æ•°æ®ç‰ˆæœ¬ã€‚æµ‹è¯•æ•°æ®ç‰ˆæœ¬æ˜¯ä¸€ä¸ªæ•°æ®ç‰ˆæœ¬ï¼Œå…¶ä¸­åŒ…å«åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒMLé—®é¢˜æ—¶ä»å®é™…ç¯å¢ƒä¸­æ”¶é›†çš„æœªè§è¿‡çš„æ–°æ•°æ®ã€‚åœ¨é‡å»ºæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸæ—¶ï¼Œä¸å¯è§çš„æµ‹è¯•æ•°æ®å°†ä½œä¸ºè®­ç»ƒæ•°æ®è¿›è¡Œæ³¨é‡Šå’Œè·¯ç”±ã€‚

A model version includes a specific ML algorithm (e.g., features transformation, model architecture) to learn from the training data. Different model versions can share some common model structure such as the same model backbone in many object detection algorithms. A training version maintains a set of training hyper-parameters used to optimize the ML model, the training logs, and the trained model. An inference version consists of deployment configurations (e.g., quantization algorithm, inference device) and the deployed model.

æ¨¡å‹ç‰ˆæœ¬åŒ…æ‹¬ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ çš„ç‰¹å®šMLç®—æ³•(ä¾‹å¦‚ï¼Œç‰¹å¾è½¬æ¢ï¼Œæ¨¡å‹æ¶æ„)ã€‚åœ¨è®¸å¤šç›®æ ‡æ£€æµ‹ç®—æ³•ä¸­ï¼Œä¸åŒçš„æ¨¡å‹ç‰ˆæœ¬å¯ä»¥å…±äº«ä¸€äº›å…±åŒçš„æ¨¡å‹ç»“æ„ï¼Œä¾‹å¦‚ç›¸åŒçš„æ¨¡å‹ä¸»å¹²ã€‚è®­ç»ƒç‰ˆæœ¬ç»´æŠ¤ä¸€ç»„è®­ç»ƒè¶…å‚æ•°ï¼Œç”¨äºä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹ã€è®­ç»ƒæ—¥å¿—å’Œè®­ç»ƒæ¨¡å‹ã€‚æ¨ç†ç‰ˆæœ¬ç”±éƒ¨ç½²é…ç½®(ä¾‹å¦‚ï¼Œé‡åŒ–ç®—æ³•ã€æ¨ç†è®¾å¤‡)å’Œéƒ¨ç½²æ¨¡å‹ç»„æˆã€‚

The core functionality of our system is the ML lifecycle version management that contains some modules, as shown in figure 2. The data version management component uses our built in-memory storage engine that can support multiple data types in a unified system, like tabular, image, and graph data. It can filter, update, add, and merge any data versions. It also supports data versions visualization and statistic functions. The model version module governs various ML model algorithms as metadata such as model backbone (e.g. ResNet50 [14]), ML architecture (e.g. FasterRCNN [34]), and so on. Thus, it provides a model versions comparison function by comparing the metadata of different ML models.

æˆ‘ä»¬ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½æ˜¯åŒ…å«ä¸€äº›æ¨¡å—çš„MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ç®¡ç†ï¼Œå¦‚å›¾2æ‰€ç¤ºã€‚æ•°æ®ç‰ˆæœ¬ç®¡ç†ç»„ä»¶ä½¿ç”¨æˆ‘ä»¬å†…ç½®çš„å†…å­˜å­˜å‚¨å¼•æ“ï¼Œè¯¥å¼•æ“å¯ä»¥åœ¨ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿä¸­æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼Œå¦‚è¡¨æ ¼ã€å›¾åƒå’Œå›¾å½¢æ•°æ®ã€‚å®ƒå¯ä»¥è¿‡æ»¤ã€æ›´æ–°ã€æ·»åŠ å’Œåˆå¹¶ä»»ä½•æ•°æ®ç‰ˆæœ¬ã€‚å®ƒè¿˜æ”¯æŒæ•°æ®ç‰ˆæœ¬å¯è§†åŒ–å’Œç»Ÿè®¡åŠŸèƒ½ã€‚æ¨¡å‹ç‰ˆæœ¬æ¨¡å—å°†å„ç§MLæ¨¡å‹ç®—æ³•ä½œä¸ºå…ƒæ•°æ®è¿›è¡Œç®¡ç†ï¼Œä¾‹å¦‚æ¨¡å‹ä¸»å¹²(ä¾‹å¦‚ResNet50[14])ã€MLæ¶æ„(ä¾‹å¦‚FasterRCNN[34])ç­‰ã€‚å› æ­¤ï¼Œå®ƒé€šè¿‡æ¯”è¾ƒä¸åŒMLæ¨¡å‹çš„å…ƒæ•°æ®æä¾›äº†æ¨¡å‹ç‰ˆæœ¬æ¯”è¾ƒåŠŸèƒ½ã€‚

![](./img/system.jpg)

å›¾äºŒï¼šç³»ç»Ÿæ¶æ„å’ŒåŠŸèƒ½  

The training version management module maintains training hyper-parameters, training logs, and the trained model of each training experiment. It provides training versions visualization and training error analysis functions. The inference version component manages deployment configurations and the deployed model of an inference version. It helps to analyze prediction errors by visualizing inference versions on real-world testing data.

è®­ç»ƒç‰ˆæœ¬ç®¡ç†æ¨¡å—ç»´æŠ¤è®­ç»ƒè¶…å‚æ•°ã€è®­ç»ƒæ—¥å¿—å’Œæ¯ä¸ªè®­ç»ƒå®éªŒçš„è®­ç»ƒæ¨¡å‹ã€‚æä¾›äº†è®­ç»ƒç‰ˆæœ¬å¯è§†åŒ–å’Œè®­ç»ƒè¯¯å·®åˆ†æåŠŸèƒ½ã€‚æ¨è®ºç‰ˆæœ¬ç»„ä»¶ç®¡ç†æ¨è®ºç‰ˆæœ¬çš„éƒ¨ç½²é…ç½®å’Œå·²éƒ¨ç½²æ¨¡å‹ã€‚å®ƒé€šè¿‡å¯è§†åŒ–çœŸå®æµ‹è¯•æ•°æ®ä¸Šçš„æ¨ç†ç‰ˆæœ¬æ¥å¸®åŠ©åˆ†æé¢„æµ‹é”™è¯¯ã€‚

Each version management module supplies an application programming interface (API) that accepts a version value and returns the data and metadata information maintained by that component. Therefore, we can build end-to-end ML lifecycle functions over our version management using their APIs (see figure 2). This research introduces our implementation for two functions: ML lifecycle transferring and automatic ML lifecycle rebuilding.

æ¯ä¸ªç‰ˆæœ¬ç®¡ç†æ¨¡å—æä¾›ä¸€ä¸ªåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£(API)ï¼Œè¯¥æ¥å£æ¥å—ç‰ˆæœ¬å€¼å¹¶è¿”å›ç”±è¯¥ç»„ä»¶ç»´æŠ¤çš„æ•°æ®å’Œå…ƒæ•°æ®ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»–ä»¬çš„apiåœ¨æˆ‘ä»¬çš„ç‰ˆæœ¬ç®¡ç†ä¸Šæ„å»ºç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸåŠŸèƒ½(è§å›¾2)ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†æˆ‘ä»¬å¯¹ä¸¤ä¸ªåŠŸèƒ½çš„å®ç°ï¼šæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸè½¬ç§»å’Œæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸè‡ªåŠ¨é‡å»ºã€‚

Firstly, we implement the ML lifecycle transferring function by reusing each lifecycle version, from model to inference, for new training data. Thanks to the APIs of each version management module, it is easy to get each versionâ€™s data and information and transfer them for the new lifecycle.

é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡é‡ç”¨ä»æ¨¡å‹åˆ°æ¨ç†çš„æ¯ä¸ªç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬æ¥å®ç°MLç”Ÿå‘½å‘¨æœŸä¼ é€’å‡½æ•°ï¼Œç”¨äºæ–°çš„è®­ç»ƒæ•°æ®ã€‚ç”±äºæ¯ä¸ªç‰ˆæœ¬ç®¡ç†æ¨¡å—çš„APIï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°è·å–æ¯ä¸ªç‰ˆæœ¬çš„æ•°æ®å’Œä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¼ è¾“åˆ°æ–°çš„ç”Ÿå‘½å‘¨æœŸã€‚

Secondly, the automatic ML lifecycle rebuilding function is performed by implementing incremental learning methods on the previous lifecycle version. For example, in the full training method, we merge the new testing data version with the previous training data version to be full training data (thanks to our data version management). Then we can reuse the previous model and training versions to train on new training data for a new ML lifecycle.

å…¶æ¬¡ï¼Œé€šè¿‡åœ¨ä¹‹å‰çš„ç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬ä¸Šå®ç°å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œå®ç°MLç”Ÿå‘½å‘¨æœŸçš„è‡ªåŠ¨é‡å»ºåŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨å®Œæ•´è®­ç»ƒæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†æ–°çš„æµ‹è¯•æ•°æ®ç‰ˆæœ¬ä¸ä»¥å‰çš„è®­ç»ƒæ•°æ®ç‰ˆæœ¬åˆå¹¶ä¸ºå®Œæ•´çš„è®­ç»ƒæ•°æ®(æ„Ÿè°¢æˆ‘ä»¬çš„æ•°æ®ç‰ˆæœ¬ç®¡ç†)ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é‡ç”¨ä»¥å‰çš„æ¨¡å‹å’Œè®­ç»ƒç‰ˆæœ¬ï¼Œä¸ºæ–°çš„MLç”Ÿå‘½å‘¨æœŸè®­ç»ƒæ–°çš„è®­ç»ƒæ•°æ®ã€‚


C. SYSTEM IMPLEMENTATION

Our critical objective is to manage all the data and training logs for numerous end-to-end ML lifecycle versions. We store the data samples, training logs, and other management information in our in-memory storage engine as insertedonly tables without deletion or modification operations. This implementation makes it easier to manage and faster to select. Particularly, we store data records of each data version in one consecutive range of storage that helps to retrieve any data version constantly, which is a benefit for reproducing a training experiment at any time in the ML lifecycle.

æˆ‘ä»¬çš„å…³é”®ç›®æ ‡æ˜¯ç®¡ç†ä¼—å¤šç«¯åˆ°ç«¯MLç”Ÿå‘½å‘¨æœŸç‰ˆæœ¬çš„æ‰€æœ‰æ•°æ®å’Œè®­ç»ƒæ—¥å¿—ã€‚æˆ‘ä»¬å°†æ•°æ®æ ·æœ¬ã€è®­ç»ƒæ—¥å¿—å’Œå…¶ä»–ç®¡ç†ä¿¡æ¯å­˜å‚¨åœ¨å†…å­˜å­˜å‚¨å¼•æ“ä¸­ï¼Œä½œä¸ºåªæ’å…¥çš„è¡¨ï¼Œä¸è¿›è¡Œåˆ é™¤æˆ–ä¿®æ”¹æ“ä½œã€‚è¿™ç§å®ç°ä½¿å®ƒæ›´å®¹æ˜“ç®¡ç†å’Œæ›´å¿«åœ°é€‰æ‹©ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ•°æ®ç‰ˆæœ¬çš„æ•°æ®è®°å½•å­˜å‚¨åœ¨ä¸€ä¸ªè¿ç»­çš„å­˜å‚¨èŒƒå›´ä¸­ï¼Œè¿™æœ‰åŠ©äºä¸æ–­æ£€ç´¢ä»»ä½•æ•°æ®ç‰ˆæœ¬ï¼Œè¿™å¯¹äºåœ¨MLç”Ÿå‘½å‘¨æœŸä¸­çš„ä»»ä½•æ—¶é—´å¤åˆ¶è®­ç»ƒå®éªŒéƒ½æ˜¯æœ‰ç›Šçš„ã€‚

Data samples and annotations are stored separately in different tables linked by data samples identifications (data IDs) which are indexed. Thus, it is flexible to manage various types of annotations, such as classes, bounding boxes, segmentation, skeletons, or adding a new one.

æ•°æ®æ ·æœ¬å’Œæ³¨é‡Šåˆ†åˆ«å­˜å‚¨åœ¨é€šè¿‡ç´¢å¼•çš„æ•°æ®æ ·æœ¬æ ‡è¯†(æ•°æ®id)é“¾æ¥çš„ä¸åŒè¡¨ä¸­ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥çµæ´»åœ°ç®¡ç†å„ç§ç±»å‹çš„æ³¨é‡Šï¼Œä¾‹å¦‚ç±»ã€è¾¹ç•Œæ¡†ã€åˆ†å‰²ã€éª¨æ¶æˆ–æ·»åŠ æ–°æ³¨é‡Šã€‚

Versioning information of a data version is organized in the graph-based schema, with each version being a node in the graph. The set of data IDs for a data version is directly stored in each node, which helps us easily extend or merge any data versions by adding or joining some sets of data IDs.

æ•°æ®ç‰ˆæœ¬çš„ç‰ˆæœ¬ä¿¡æ¯ç»„ç»‡åœ¨åŸºäºå›¾çš„æ¨¡å¼ä¸­ï¼Œæ¯ä¸ªç‰ˆæœ¬æ˜¯å›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚æ•°æ®ç‰ˆæœ¬çš„æ•°æ®idé›†ç›´æ¥å­˜å‚¨åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸­ï¼Œè¿™æœ‰åŠ©äºæˆ‘ä»¬é€šè¿‡æ·»åŠ æˆ–è¿æ¥ä¸€äº›æ•°æ®idé›†è½»æ¾æ‰©å±•æˆ–åˆå¹¶ä»»ä½•æ•°æ®ç‰ˆæœ¬ã€‚

Moreover, ML configurations like model algorithms and training hyper-parameters are represented as metadata nodes in the graph. All ML versions, like model, training, and inference versions, are also managed in the graph. An ML version with a relationship with each other, such as a model version that is fine-tuned from other model versions, can be denoted as links in our graph management.

æ­¤å¤–ï¼Œåƒæ¨¡å‹ç®—æ³•å’Œè®­ç»ƒè¶…å‚æ•°è¿™æ ·çš„æœºå™¨å­¦ä¹ é…ç½®è¢«è¡¨ç¤ºä¸ºå›¾ä¸­çš„å…ƒæ•°æ®èŠ‚ç‚¹ã€‚æ‰€æœ‰MLç‰ˆæœ¬ï¼Œå¦‚æ¨¡å‹ã€è®­ç»ƒå’Œæ¨ç†ç‰ˆæœ¬ï¼Œä¹Ÿåœ¨å›¾ä¸­è¿›è¡Œç®¡ç†ã€‚å…·æœ‰ç›¸äº’å…³ç³»çš„MLç‰ˆæœ¬ï¼Œä¾‹å¦‚ä»å…¶ä»–æ¨¡å‹ç‰ˆæœ¬å¾®è°ƒçš„æ¨¡å‹ç‰ˆæœ¬ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„å›¾ç®¡ç†ä¸­è¡¨ç¤ºä¸ºé“¾æ¥ã€‚

Figure 3 illustrates how we organize model versions, model metadata, and their relationships in graph-based management. Using graph representation, we can easily inspect an ML lifecycle through any ML version (data to inference) and at any time.

å›¾3è¯´æ˜äº†æˆ‘ä»¬å¦‚ä½•åœ¨åŸºäºå›¾çš„ç®¡ç†ä¸­ç»„ç»‡æ¨¡å‹ç‰ˆæœ¬ã€æ¨¡å‹å…ƒæ•°æ®ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ä½¿ç”¨å›¾è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ—¶é—´é€šè¿‡ä»»ä½•MLç‰ˆæœ¬(ä»æ•°æ®åˆ°æ¨ç†)è½»æ¾åœ°æ£€æŸ¥MLç”Ÿå‘½å‘¨æœŸã€‚

![](./img/model.jpg)

å›¾3ï¼šç³»ç»Ÿä¸­æ¨¡å‹ç‰ˆæœ¬å’Œæ¨¡å‹å…ƒæ•°æ®ï¼ˆåŸºäºå›¾çš„ç®¡ç†ï¼‰ã€‚

